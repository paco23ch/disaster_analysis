{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "x_columns = ['id','message','original','genre']\n",
    "engine = create_engine('sqlite:///messages_clean.db')\n",
    "df = pd.read_sql_table('SampleTable',engine)\n",
    "X = df['message']\n",
    "Y = df.drop(x_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(token).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "        \n",
    "    return(clean_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('multi', MultiOutputClassifier(estimator=RandomForestClassifier()))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 1. Classification report for related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.35      0.45      1206\n",
      "          1       0.82      0.93      0.87      4000\n",
      "          2       0.33      0.08      0.13        38\n",
      "\n",
      "avg / total       0.77      0.79      0.77      5244\n",
      "\n",
      "*** 2. Classification report for request\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.99      0.93      4329\n",
      "          1       0.87      0.36      0.51       915\n",
      "\n",
      "avg / total       0.88      0.88      0.86      5244\n",
      "\n",
      "*** 3. Classification report for offer\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00      5216\n",
      "          1       0.00      0.00      0.00        28\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5244\n",
      "\n",
      "*** 4. Classification report for aid_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.89      0.79      3019\n",
      "          1       0.77      0.53      0.63      2225\n",
      "\n",
      "avg / total       0.74      0.73      0.72      5244\n",
      "\n",
      "*** 5. Classification report for medical_help\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96      4812\n",
      "          1       0.59      0.07      0.12       432\n",
      "\n",
      "avg / total       0.90      0.92      0.89      5244\n",
      "\n",
      "*** 6. Classification report for medical_products\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      4971\n",
      "          1       0.64      0.05      0.09       273\n",
      "\n",
      "avg / total       0.93      0.95      0.93      5244\n",
      "\n",
      "*** 7. Classification report for search_and_rescue\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99      5094\n",
      "          1       0.40      0.01      0.03       150\n",
      "\n",
      "avg / total       0.96      0.97      0.96      5244\n",
      "\n",
      "*** 8. Classification report for security\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      5161\n",
      "          1       0.00      0.00      0.00        83\n",
      "\n",
      "avg / total       0.97      0.98      0.98      5244\n",
      "\n",
      "*** 9. Classification report for military\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      5069\n",
      "          1       0.38      0.03      0.05       175\n",
      "\n",
      "avg / total       0.95      0.97      0.95      5244\n",
      "\n",
      "*** 10. Classification report for child_alone\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      5244\n",
      "\n",
      "avg / total       1.00      1.00      1.00      5244\n",
      "\n",
      "*** 11. Classification report for water\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97      4899\n",
      "          1       0.92      0.13      0.23       345\n",
      "\n",
      "avg / total       0.94      0.94      0.92      5244\n",
      "\n",
      "*** 12. Classification report for food\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.99      0.95      4658\n",
      "          1       0.86      0.26      0.40       586\n",
      "\n",
      "avg / total       0.91      0.91      0.89      5244\n",
      "\n",
      "*** 13. Classification report for shelter\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96      4762\n",
      "          1       0.84      0.22      0.34       482\n",
      "\n",
      "avg / total       0.92      0.92      0.90      5244\n",
      "\n",
      "*** 14. Classification report for clothing\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5165\n",
      "          1       0.67      0.10      0.18        79\n",
      "\n",
      "avg / total       0.98      0.99      0.98      5244\n",
      "\n",
      "*** 15. Classification report for money\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      5124\n",
      "          1       0.86      0.05      0.09       120\n",
      "\n",
      "avg / total       0.98      0.98      0.97      5244\n",
      "\n",
      "*** 16. Classification report for missing_people\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5183\n",
      "          1       0.00      0.00      0.00        61\n",
      "\n",
      "avg / total       0.98      0.99      0.98      5244\n",
      "\n",
      "*** 17. Classification report for refugees\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      5047\n",
      "          1       0.50      0.04      0.07       197\n",
      "\n",
      "avg / total       0.95      0.96      0.95      5244\n",
      "\n",
      "*** 18. Classification report for death\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      4986\n",
      "          1       0.90      0.14      0.24       258\n",
      "\n",
      "avg / total       0.95      0.96      0.94      5244\n",
      "\n",
      "*** 19. Classification report for other_aid\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.99      0.93      4565\n",
      "          1       0.47      0.04      0.07       679\n",
      "\n",
      "avg / total       0.82      0.87      0.82      5244\n",
      "\n",
      "*** 20. Classification report for infrastructure_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96      4834\n",
      "          1       0.50      0.00      0.00       410\n",
      "\n",
      "avg / total       0.89      0.92      0.88      5244\n",
      "\n",
      "*** 21. Classification report for transport\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98      4985\n",
      "          1       0.67      0.05      0.10       259\n",
      "\n",
      "avg / total       0.94      0.95      0.93      5244\n",
      "\n",
      "*** 22. Classification report for buildings\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      4953\n",
      "          1       0.70      0.11      0.20       291\n",
      "\n",
      "avg / total       0.94      0.95      0.93      5244\n",
      "\n",
      "*** 23. Classification report for electricity\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      5138\n",
      "          1       0.73      0.08      0.14       106\n",
      "\n",
      "avg / total       0.98      0.98      0.97      5244\n",
      "\n",
      "*** 24. Classification report for tools\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00      5214\n",
      "          1       0.00      0.00      0.00        30\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5244\n",
      "\n",
      "*** 25. Classification report for hospitals\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5173\n",
      "          1       0.00      0.00      0.00        71\n",
      "\n",
      "avg / total       0.97      0.99      0.98      5244\n",
      "\n",
      "*** 26. Classification report for shops\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      5223\n",
      "          1       0.00      0.00      0.00        21\n",
      "\n",
      "avg / total       0.99      1.00      0.99      5244\n",
      "\n",
      "*** 27. Classification report for aid_centers\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5171\n",
      "          1       0.00      0.00      0.00        73\n",
      "\n",
      "avg / total       0.97      0.99      0.98      5244\n",
      "\n",
      "*** 28. Classification report for other_infrastructure\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      4970\n",
      "          1       0.40      0.01      0.01       274\n",
      "\n",
      "avg / total       0.92      0.95      0.92      5244\n",
      "\n",
      "*** 29. Classification report for weather_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.97      0.89      3739\n",
      "          1       0.86      0.50      0.63      1505\n",
      "\n",
      "avg / total       0.84      0.83      0.82      5244\n",
      "\n",
      "*** 30. Classification report for floods\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97      4787\n",
      "          1       0.92      0.28      0.43       457\n",
      "\n",
      "avg / total       0.93      0.93      0.92      5244\n",
      "\n",
      "*** 31. Classification report for storm\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.96      4774\n",
      "          1       0.74      0.32      0.45       470\n",
      "\n",
      "avg / total       0.92      0.93      0.92      5244\n",
      "\n",
      "*** 32. Classification report for fire\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5181\n",
      "          1       0.00      0.00      0.00        63\n",
      "\n",
      "avg / total       0.98      0.99      0.98      5244\n",
      "\n",
      "*** 33. Classification report for earthquake\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.99      0.97      4736\n",
      "          1       0.89      0.48      0.63       508\n",
      "\n",
      "avg / total       0.94      0.94      0.94      5244\n",
      "\n",
      "*** 34. Classification report for cold\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      5130\n",
      "          1       0.44      0.04      0.07       114\n",
      "\n",
      "avg / total       0.97      0.98      0.97      5244\n",
      "\n",
      "*** 35. Classification report for other_weather\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97      4949\n",
      "          1       0.25      0.01      0.01       295\n",
      "\n",
      "avg / total       0.90      0.94      0.92      5244\n",
      "\n",
      "*** 36. Classification report for direct_report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.98      0.91      4200\n",
      "          1       0.80      0.30      0.44      1044\n",
      "\n",
      "avg / total       0.84      0.85      0.82      5244\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "for i,col in enumerate(y_test.columns):\n",
    "    y_pred_i = y_pred.T[i]\n",
    "    y_test_i = y_test[col]\n",
    "    print('*** {}. Classification report for {}'.format(i+1,col))\n",
    "    print(classification_report(y_test_i, y_pred_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7ff15003c268>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('multi',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7ff15003c268>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'multi': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'multi__estimator__bootstrap': True,\n",
       " 'multi__estimator__class_weight': None,\n",
       " 'multi__estimator__criterion': 'gini',\n",
       " 'multi__estimator__max_depth': None,\n",
       " 'multi__estimator__max_features': 'auto',\n",
       " 'multi__estimator__max_leaf_nodes': None,\n",
       " 'multi__estimator__min_impurity_decrease': 0.0,\n",
       " 'multi__estimator__min_impurity_split': None,\n",
       " 'multi__estimator__min_samples_leaf': 1,\n",
       " 'multi__estimator__min_samples_split': 2,\n",
       " 'multi__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'multi__estimator__n_estimators': 10,\n",
       " 'multi__estimator__n_jobs': 1,\n",
       " 'multi__estimator__oob_score': False,\n",
       " 'multi__estimator__random_state': None,\n",
       " 'multi__estimator__verbose': 0,\n",
       " 'multi__estimator__warm_start': False,\n",
       " 'multi__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " 'multi__n_jobs': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Pipeline(steps=[('vect',\n",
    "                 CountVectorizer(tokenizer=<function tokenize at 0x7fb45020e9e0>)),\n",
    "                ('tfidf', TfidfTransformer(norm='l1', smooth_idf=False)),\n",
    "                ('multi',\n",
    "                 MultiOutputClassifier(estimator=RandomForestClassifier()))],\n",
    "         verbose=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.1988270633671864, total=  20.9s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   32.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.19997139178944356, total=  20.8s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.19399141630901287, total=  21.1s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.1991131454727507, total=  31.0s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  2.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.19997139178944356, total=  31.0s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.19399141630901287, total=  30.6s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  3.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.1991131454727507, total=  49.7s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  5.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.19997139178944356, total=  50.4s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  6.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.19399141630901287, total=  50.2s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  7.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.19897010441996854, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  9.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.19997139178944356, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed: 10.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.19399141630901287, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed: 12.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.19897010441996854, total=  20.7s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed: 13.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.19968530968387926, total=  21.0s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed: 13.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.19399141630901287, total=  21.0s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.1991131454727507, total=  30.9s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.19997139178944356, total=  30.2s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.19399141630901287, total=  30.4s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.1991131454727507, total=  49.6s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.19982835073666141, total=  49.2s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.19399141630901287, total=  49.6s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.1991131454727507, total= 1.2min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.19997139178944356, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.19399141630901287, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.19868402231440424, total=  21.0s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.19982835073666141, total=  20.9s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.19399141630901287, total=  21.1s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.1988270633671864, total=  30.6s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.19997139178944356, total=  30.5s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.19399141630901287, total=  30.7s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.19897010441996854, total=  51.7s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.19982835073666141, total=  49.8s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.19399141630901287, total=  49.3s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.1991131454727507, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.19997139178944356, total= 1.2min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.19399141630901287, total= 1.2min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.1979688170504935, total=  22.8s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.19968530968387926, total=  23.1s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.19341917024320457, total=  22.5s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.19868402231440424, total=  33.4s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.19982835073666141, total=  33.5s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.19341917024320457, total=  33.3s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.19839794020883994, total=  54.6s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.19982835073666141, total=  55.3s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.19370529327610872, total=  54.6s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.1985409812616221, total= 1.3min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.19968530968387926, total= 1.3min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.19399141630901287, total= 1.3min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.1979688170504935, total=  22.3s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.19968530968387926, total=  22.3s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.19370529327610872, total=  22.1s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.1988270633671864, total=  32.6s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.19939922757831496, total=  33.2s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.19341917024320457, total=  32.8s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.19868402231440424, total=  54.5s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.1995422686310971, total=  54.2s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.19399141630901287, total=  54.4s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.19839794020883994, total= 1.3min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.19968530968387926, total= 1.3min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.19399141630901287, total= 1.2min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.1985409812616221, total=  21.8s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.1991131454727507, total=  22.0s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.19370529327610872, total=  21.9s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.1985409812616221, total=  32.3s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.19968530968387926, total=  32.3s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.19399141630901287, total=  32.4s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.1985409812616221, total=  54.0s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.19968530968387926, total=  54.2s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.1938483547925608, total=  55.1s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.1985409812616221, total= 1.3min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.19968530968387926, total= 1.3min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.19399141630901287, total= 1.3min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.19782577599771134, total=  23.8s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.1991131454727507, total=  23.5s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.19341917024320457, total=  23.4s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.19782577599771134, total=  36.0s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.19897010441996854, total=  35.9s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.19241773962804007, total=  35.9s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.19811185810327564, total= 1.0min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.19939922757831496, total= 1.0min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.1938483547925608, total= 1.0min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.1976827349449292, total= 1.4min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.1991131454727507, total= 1.4min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.19370529327610872, total= 1.4min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.1973966528393649, total=  23.6s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.1991131454727507, total=  23.5s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.1932761087267525, total=  23.3s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.1982548991560578, total=  35.4s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.19839794020883994, total=  35.7s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.19313304721030042, total=  37.2s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.1973966528393649, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.19939922757831496, total= 1.0min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.19313304721030042, total= 1.0min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.1976827349449292, total= 1.4min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.19925618652553284, total= 1.4min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.1932761087267525, total= 1.4min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.1982548991560578, total=  23.4s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.19925618652553284, total=  23.2s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.19341917024320457, total=  23.2s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.19782577599771134, total=  35.2s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.19897010441996854, total=  35.0s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.19341917024320457, total=  35.6s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.19782577599771134, total=  59.0s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.19897010441996854, total=  59.2s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.19356223175965664, total=  59.2s\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.1979688170504935, total= 1.4min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.19897010441996854, total= 1.4min\n",
      "[CV] multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=True, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.19341917024320457, total= 1.4min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.1988270633671864, total=  21.4s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.1995422686310971, total=  21.7s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.19399141630901287, total=  21.6s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.1991131454727507, total=  31.8s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.19982835073666141, total=  31.9s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.19399141630901287, total=  32.0s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.19897010441996854, total=  53.0s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.19997139178944356, total=  52.5s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.19399141630901287, total=  52.1s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.1991131454727507, total= 1.2min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.19997139178944356, total= 1.2min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.19399141630901287, total= 1.2min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.1991131454727507, total=  21.7s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.19997139178944356, total=  21.8s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.19399141630901287, total=  22.1s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.1991131454727507, total=  32.5s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.19997139178944356, total=  32.6s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.19399141630901287, total=  38.4s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.19868402231440424, total=  59.7s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.19997139178944356, total= 1.0min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.19399141630901287, total=  57.7s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.1991131454727507, total= 1.5min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.19997139178944356, total= 1.3min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.19399141630901287, total= 1.3min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.19897010441996854, total=  23.5s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.19997139178944356, total=  23.6s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.19399141630901287, total=  24.3s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.19897010441996854, total=  35.2s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.19968530968387926, total=  32.6s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.19399141630901287, total=  32.3s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.19897010441996854, total=  53.2s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.19997139178944356, total=  54.8s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.19399141630901287, total=  52.6s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.19897010441996854, total= 1.2min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.19997139178944356, total= 1.2min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=5, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.19399141630901287, total= 1.2min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.19839794020883994, total=  23.8s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.19939922757831496, total=  23.9s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.19313304721030042, total=  24.1s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.1985409812616221, total=  36.6s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.1995422686310971, total=  37.2s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.19370529327610872, total=  38.3s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.1985409812616221, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.19968530968387926, total= 1.0min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.19399141630901287, total= 1.0min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.1985409812616221, total= 1.5min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.19968530968387926, total= 1.5min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.1938483547925608, total= 1.4min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.19839794020883994, total=  23.9s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.19925618652553284, total=  23.7s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.19341917024320457, total=  23.6s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.19839794020883994, total=  36.0s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.1995422686310971, total=  36.1s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.1938483547925608, total=  36.6s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.19839794020883994, total= 1.0min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.19939922757831496, total= 1.0min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.1938483547925608, total= 1.0min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.19868402231440424, total= 1.4min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.19939922757831496, total= 1.4min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.19399141630901287, total= 1.6min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.19811185810327564, total=  29.1s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.1988270633671864, total=  28.0s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.19370529327610872, total=  27.3s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.19839794020883994, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.19982835073666141, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.19356223175965664, total=  44.6s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.19839794020883994, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.19939922757831496, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.19399141630901287, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.1985409812616221, total= 1.5min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.19982835073666141, total= 1.5min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=7, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.1938483547925608, total= 1.4min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.19568016020597911, total=  26.3s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.1982548991560578, total=  25.4s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=50, score=0.19284692417739627, total=  25.5s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.19753969389214704, total=  40.2s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.1988270633671864, total=  40.4s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=100, score=0.1938483547925608, total=  39.6s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.1979688170504935, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.19897010441996854, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=200, score=0.1932761087267525, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.1982548991560578, total= 1.6min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.1991131454727507, total= 1.6min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=3, multi__estimator__n_estimators=300, score=0.19341917024320457, total= 1.6min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.1976827349449292, total=  25.9s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.19897010441996854, total=  25.7s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=50, score=0.19341917024320457, total=  25.6s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.19782577599771134, total=  40.2s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.1988270633671864, total=  39.9s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=100, score=0.19341917024320457, total=  39.8s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.1979688170504935, total= 1.2min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.1991131454727507, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=200, score=0.19341917024320457, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.1976827349449292, total= 1.6min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.1991131454727507, total= 1.6min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=5, multi__estimator__n_estimators=300, score=0.19370529327610872, total= 1.6min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.19753969389214704, total=  25.4s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.1988270633671864, total=  26.0s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=50, score=0.19370529327610872, total=  25.9s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.1973966528393649, total=  40.0s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.19868402231440424, total=  39.7s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=100, score=0.19313304721030042, total=  40.0s\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.19811185810327564, total= 1.2min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.19925618652553284, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=200, score=0.19313304721030042, total= 1.1min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.19811185810327564, total= 1.6min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.19868402231440424, total= 1.6min\n",
      "[CV] multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__bootstrap=False, multi__estimator__max_depth=9, multi__estimator__min_samples_leaf=7, multi__estimator__n_estimators=300, score=0.1932761087267525, total= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 216 out of 216 | elapsed: 253.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'multi__estimator__bootstrap': [True, False], 'multi__estimator__max_depth': [5, 7, 9], 'multi__estimator__min_samples_leaf': [3, 5, 7], 'multi__estimator__n_estimators': [50, 100, 200, 300]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'multi__estimator__bootstrap' : [True, False],\n",
    "              'multi__estimator__max_depth' : [5, 7, 9],\n",
    "              'multi__estimator__min_samples_leaf' : [3, 5, 7],\n",
    "              'multi__estimator__n_estimators' : [50, 100, 200, 300]}\n",
    "\n",
    "cv = GridSearchCV(pipeline,param_grid=parameters, verbose=15)\n",
    "cv.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 1. Classification report for related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1206\n",
      "          1       0.76      1.00      0.87      4000\n",
      "          2       0.00      0.00      0.00        38\n",
      "\n",
      "avg / total       0.58      0.76      0.66      5244\n",
      "\n",
      "*** 2. Classification report for request\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      1.00      0.90      4329\n",
      "          1       0.00      0.00      0.00       915\n",
      "\n",
      "avg / total       0.68      0.83      0.75      5244\n",
      "\n",
      "*** 3. Classification report for offer\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00      5216\n",
      "          1       0.00      0.00      0.00        28\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5244\n",
      "\n",
      "*** 4. Classification report for aid_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      1.00      0.73      3019\n",
      "          1       1.00      0.01      0.02      2225\n",
      "\n",
      "avg / total       0.76      0.58      0.43      5244\n",
      "\n",
      "*** 5. Classification report for medical_help\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96      4812\n",
      "          1       0.00      0.00      0.00       432\n",
      "\n",
      "avg / total       0.84      0.92      0.88      5244\n",
      "\n",
      "*** 6. Classification report for medical_products\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      4971\n",
      "          1       0.00      0.00      0.00       273\n",
      "\n",
      "avg / total       0.90      0.95      0.92      5244\n",
      "\n",
      "*** 7. Classification report for search_and_rescue\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99      5094\n",
      "          1       0.00      0.00      0.00       150\n",
      "\n",
      "avg / total       0.94      0.97      0.96      5244\n",
      "\n",
      "*** 8. Classification report for security\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      5161\n",
      "          1       0.00      0.00      0.00        83\n",
      "\n",
      "avg / total       0.97      0.98      0.98      5244\n",
      "\n",
      "*** 9. Classification report for military\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      5069\n",
      "          1       0.00      0.00      0.00       175\n",
      "\n",
      "avg / total       0.93      0.97      0.95      5244\n",
      "\n",
      "*** 10. Classification report for child_alone\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      5244\n",
      "\n",
      "avg / total       1.00      1.00      1.00      5244\n",
      "\n",
      "*** 11. Classification report for water\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.97      4899\n",
      "          1       0.00      0.00      0.00       345\n",
      "\n",
      "avg / total       0.87      0.93      0.90      5244\n",
      "\n",
      "*** 12. Classification report for food\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94      4658\n",
      "          1       0.00      0.00      0.00       586\n",
      "\n",
      "avg / total       0.79      0.89      0.84      5244\n",
      "\n",
      "*** 13. Classification report for shelter\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95      4762\n",
      "          1       0.00      0.00      0.00       482\n",
      "\n",
      "avg / total       0.82      0.91      0.86      5244\n",
      "\n",
      "*** 14. Classification report for clothing\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      5165\n",
      "          1       0.00      0.00      0.00        79\n",
      "\n",
      "avg / total       0.97      0.98      0.98      5244\n",
      "\n",
      "*** 15. Classification report for money\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      5124\n",
      "          1       0.00      0.00      0.00       120\n",
      "\n",
      "avg / total       0.95      0.98      0.97      5244\n",
      "\n",
      "*** 16. Classification report for missing_people\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5183\n",
      "          1       0.00      0.00      0.00        61\n",
      "\n",
      "avg / total       0.98      0.99      0.98      5244\n",
      "\n",
      "*** 17. Classification report for refugees\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      5047\n",
      "          1       0.00      0.00      0.00       197\n",
      "\n",
      "avg / total       0.93      0.96      0.94      5244\n",
      "\n",
      "*** 18. Classification report for death\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      4986\n",
      "          1       0.00      0.00      0.00       258\n",
      "\n",
      "avg / total       0.90      0.95      0.93      5244\n",
      "\n",
      "*** 19. Classification report for other_aid\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93      4565\n",
      "          1       0.00      0.00      0.00       679\n",
      "\n",
      "avg / total       0.76      0.87      0.81      5244\n",
      "\n",
      "*** 20. Classification report for infrastructure_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96      4834\n",
      "          1       0.00      0.00      0.00       410\n",
      "\n",
      "avg / total       0.85      0.92      0.88      5244\n",
      "\n",
      "*** 21. Classification report for transport\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      4985\n",
      "          1       0.00      0.00      0.00       259\n",
      "\n",
      "avg / total       0.90      0.95      0.93      5244\n",
      "\n",
      "*** 22. Classification report for buildings\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97      4953\n",
      "          1       0.00      0.00      0.00       291\n",
      "\n",
      "avg / total       0.89      0.94      0.92      5244\n",
      "\n",
      "*** 23. Classification report for electricity\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      5138\n",
      "          1       0.00      0.00      0.00       106\n",
      "\n",
      "avg / total       0.96      0.98      0.97      5244\n",
      "\n",
      "*** 24. Classification report for tools\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00      5214\n",
      "          1       0.00      0.00      0.00        30\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5244\n",
      "\n",
      "*** 25. Classification report for hospitals\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5173\n",
      "          1       0.00      0.00      0.00        71\n",
      "\n",
      "avg / total       0.97      0.99      0.98      5244\n",
      "\n",
      "*** 26. Classification report for shops\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      5223\n",
      "          1       0.00      0.00      0.00        21\n",
      "\n",
      "avg / total       0.99      1.00      0.99      5244\n",
      "\n",
      "*** 27. Classification report for aid_centers\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5171\n",
      "          1       0.00      0.00      0.00        73\n",
      "\n",
      "avg / total       0.97      0.99      0.98      5244\n",
      "\n",
      "*** 28. Classification report for other_infrastructure\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      4970\n",
      "          1       0.00      0.00      0.00       274\n",
      "\n",
      "avg / total       0.90      0.95      0.92      5244\n",
      "\n",
      "*** 29. Classification report for weather_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      1.00      0.83      3739\n",
      "          1       1.00      0.01      0.01      1505\n",
      "\n",
      "avg / total       0.80      0.71      0.60      5244\n",
      "\n",
      "*** 30. Classification report for floods\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95      4787\n",
      "          1       0.00      0.00      0.00       457\n",
      "\n",
      "avg / total       0.83      0.91      0.87      5244\n",
      "\n",
      "*** 31. Classification report for storm\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95      4774\n",
      "          1       1.00      0.00      0.00       470\n",
      "\n",
      "avg / total       0.92      0.91      0.87      5244\n",
      "\n",
      "*** 32. Classification report for fire\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5181\n",
      "          1       0.00      0.00      0.00        63\n",
      "\n",
      "avg / total       0.98      0.99      0.98      5244\n",
      "\n",
      "*** 33. Classification report for earthquake\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.95      4736\n",
      "          1       0.00      0.00      0.00       508\n",
      "\n",
      "avg / total       0.82      0.90      0.86      5244\n",
      "\n",
      "*** 34. Classification report for cold\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      5130\n",
      "          1       0.00      0.00      0.00       114\n",
      "\n",
      "avg / total       0.96      0.98      0.97      5244\n",
      "\n",
      "*** 35. Classification report for other_weather\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97      4949\n",
      "          1       0.00      0.00      0.00       295\n",
      "\n",
      "avg / total       0.89      0.94      0.92      5244\n",
      "\n",
      "*** 36. Classification report for direct_report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      1.00      0.89      4200\n",
      "          1       0.00      0.00      0.00      1044\n",
      "\n",
      "avg / total       0.64      0.80      0.71      5244\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = cv.best_estimator_.predict(X_test)\n",
    "\n",
    "for i,col in enumerate(y_test.columns):\n",
    "    y_pred_i = y_pred.T[i]\n",
    "    y_test_i = y_test[col]\n",
    "    print('*** {}. Classification report for {}'.format(i+1,col))\n",
    "    print(classification_report(y_test_i, y_pred_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternate_pipeline = Pipeline([('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('multi', MultiOutputClassifier(estimator=AdaBoostClassifier()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "model2 = alternate_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 1. Classification report for related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.31      0.41      1206\n",
      "          1       0.81      0.94      0.87      4002\n",
      "          2       0.00      0.00      0.00        36\n",
      "\n",
      "avg / total       0.76      0.79      0.76      5244\n",
      "\n",
      "*** 2. Classification report for request\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.96      0.93      4317\n",
      "          1       0.76      0.54      0.63       927\n",
      "\n",
      "avg / total       0.88      0.89      0.88      5244\n",
      "\n",
      "*** 3. Classification report for offer\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      5220\n",
      "          1       0.00      0.00      0.00        24\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5244\n",
      "\n",
      "*** 4. Classification report for aid_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.87      0.80      3003\n",
      "          1       0.77      0.60      0.67      2241\n",
      "\n",
      "avg / total       0.75      0.75      0.75      5244\n",
      "\n",
      "*** 5. Classification report for medical_help\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.96      4817\n",
      "          1       0.62      0.27      0.38       427\n",
      "\n",
      "avg / total       0.91      0.93      0.91      5244\n",
      "\n",
      "*** 6. Classification report for medical_products\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.98      4966\n",
      "          1       0.68      0.33      0.44       278\n",
      "\n",
      "avg / total       0.95      0.96      0.95      5244\n",
      "\n",
      "*** 7. Classification report for search_and_rescue\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      5094\n",
      "          1       0.64      0.19      0.30       150\n",
      "\n",
      "avg / total       0.97      0.97      0.97      5244\n",
      "\n",
      "*** 8. Classification report for security\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      5140\n",
      "          1       0.17      0.02      0.03       104\n",
      "\n",
      "avg / total       0.96      0.98      0.97      5244\n",
      "\n",
      "*** 9. Classification report for military\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99      5069\n",
      "          1       0.65      0.34      0.45       175\n",
      "\n",
      "avg / total       0.97      0.97      0.97      5244\n",
      "\n",
      "*** 10. Classification report for child_alone\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      5244\n",
      "\n",
      "avg / total       1.00      1.00      1.00      5244\n",
      "\n",
      "*** 11. Classification report for water\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.98      0.98      4895\n",
      "          1       0.73      0.62      0.67       349\n",
      "\n",
      "avg / total       0.96      0.96      0.96      5244\n",
      "\n",
      "*** 12. Classification report for food\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.98      0.97      4640\n",
      "          1       0.84      0.70      0.76       604\n",
      "\n",
      "avg / total       0.95      0.95      0.95      5244\n",
      "\n",
      "*** 13. Classification report for shelter\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.99      0.97      4756\n",
      "          1       0.79      0.52      0.63       488\n",
      "\n",
      "avg / total       0.94      0.94      0.94      5244\n",
      "\n",
      "*** 14. Classification report for clothing\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5151\n",
      "          1       0.82      0.45      0.58        93\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5244\n",
      "\n",
      "*** 15. Classification report for money\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99      5128\n",
      "          1       0.52      0.29      0.37       116\n",
      "\n",
      "avg / total       0.97      0.98      0.98      5244\n",
      "\n",
      "*** 16. Classification report for missing_people\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5170\n",
      "          1       0.64      0.12      0.20        74\n",
      "\n",
      "avg / total       0.98      0.99      0.98      5244\n",
      "\n",
      "*** 17. Classification report for refugees\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98      5043\n",
      "          1       0.58      0.21      0.31       201\n",
      "\n",
      "avg / total       0.95      0.96      0.96      5244\n",
      "\n",
      "*** 18. Classification report for death\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98      4978\n",
      "          1       0.79      0.48      0.60       266\n",
      "\n",
      "avg / total       0.96      0.97      0.96      5244\n",
      "\n",
      "*** 19. Classification report for other_aid\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.98      0.92      4516\n",
      "          1       0.51      0.14      0.22       728\n",
      "\n",
      "avg / total       0.83      0.86      0.83      5244\n",
      "\n",
      "*** 20. Classification report for infrastructure_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.97      4901\n",
      "          1       0.47      0.11      0.18       343\n",
      "\n",
      "avg / total       0.91      0.93      0.91      5244\n",
      "\n",
      "*** 21. Classification report for transport\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      5001\n",
      "          1       0.68      0.17      0.28       243\n",
      "\n",
      "avg / total       0.95      0.96      0.95      5244\n",
      "\n",
      "*** 22. Classification report for buildings\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98      4987\n",
      "          1       0.66      0.43      0.52       257\n",
      "\n",
      "avg / total       0.96      0.96      0.96      5244\n",
      "\n",
      "*** 23. Classification report for electricity\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5132\n",
      "          1       0.70      0.36      0.47       112\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5244\n",
      "\n",
      "*** 24. Classification report for tools\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00      5212\n",
      "          1       0.00      0.00      0.00        32\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5244\n",
      "\n",
      "*** 25. Classification report for hospitals\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5185\n",
      "          1       0.33      0.08      0.14        59\n",
      "\n",
      "avg / total       0.98      0.99      0.98      5244\n",
      "\n",
      "*** 26. Classification report for shops\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      5219\n",
      "          1       0.00      0.00      0.00        25\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5244\n",
      "\n",
      "*** 27. Classification report for aid_centers\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5165\n",
      "          1       0.50      0.14      0.22        79\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5244\n",
      "\n",
      "*** 28. Classification report for other_infrastructure\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.98      5025\n",
      "          1       0.38      0.10      0.15       219\n",
      "\n",
      "avg / total       0.94      0.96      0.94      5244\n",
      "\n",
      "*** 29. Classification report for weather_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.96      0.91      3773\n",
      "          1       0.85      0.63      0.72      1471\n",
      "\n",
      "avg / total       0.86      0.87      0.86      5244\n",
      "\n",
      "*** 30. Classification report for floods\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.97      4796\n",
      "          1       0.88      0.51      0.65       448\n",
      "\n",
      "avg / total       0.95      0.95      0.95      5244\n",
      "\n",
      "*** 31. Classification report for storm\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.97      4763\n",
      "          1       0.76      0.48      0.59       481\n",
      "\n",
      "avg / total       0.93      0.94      0.93      5244\n",
      "\n",
      "*** 32. Classification report for fire\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5181\n",
      "          1       0.60      0.24      0.34        63\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5244\n",
      "\n",
      "*** 33. Classification report for earthquake\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98      4766\n",
      "          1       0.86      0.75      0.80       478\n",
      "\n",
      "avg / total       0.96      0.97      0.97      5244\n",
      "\n",
      "*** 34. Classification report for cold\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5150\n",
      "          1       0.61      0.33      0.43        94\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5244\n",
      "\n",
      "*** 35. Classification report for other_weather\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.99      0.97      4944\n",
      "          1       0.55      0.19      0.28       300\n",
      "\n",
      "avg / total       0.93      0.94      0.93      5244\n",
      "\n",
      "*** 36. Classification report for direct_report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.95      0.92      4193\n",
      "          1       0.73      0.50      0.59      1051\n",
      "\n",
      "avg / total       0.85      0.86      0.85      5244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = alternate_pipeline.predict(X_test)\n",
    "\n",
    "for i,col in enumerate(y_test.columns):\n",
    "    y_pred_i = y_pred.T[i]\n",
    "    y_test_i = y_test[col]\n",
    "    print('*** {}. Classification report for {}'.format(i+1,col))\n",
    "    print(classification_report(y_test_i, y_pred_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7f4c20174620>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('multi',\n",
       "   MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "             learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7f4c20174620>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'multi': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'multi__estimator__algorithm': 'SAMME.R',\n",
       " 'multi__estimator__base_estimator': None,\n",
       " 'multi__estimator__learning_rate': 1.0,\n",
       " 'multi__estimator__n_estimators': 50,\n",
       " 'multi__estimator__random_state': None,\n",
       " 'multi__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None),\n",
       " 'multi__n_jobs': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternate_pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=100 \n",
      "[CV]  multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=100, score=0.23387212129881277, total= 4.0min\n",
      "[CV] multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=100, score=0.23229866971820912, total= 4.0min\n",
      "[CV] multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  8.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=100, score=0.24763948497854077, total= 4.1min\n",
      "[CV] multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=200 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 13.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=200, score=0.23773422972393077, total= 7.9min\n",
      "[CV] multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=200 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 21.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=200, score=0.24345587183521672, total= 7.9min\n",
      "[CV] multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=200 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 29.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=200, score=0.24434907010014306, total= 7.9min\n",
      "[CV] multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=300 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 38.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=300, score=0.20311829495065084, total=11.7min\n",
      "[CV] multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=300 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 50.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=300, score=0.20998426548419397, total=11.8min\n",
      "[CV] multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=300 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 63.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__learning_rate=0.9, multi__estimator__n_estimators=300, score=0.23505007153075821, total=11.8min\n",
      "[CV] multi__estimator__learning_rate=1, multi__estimator__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 75.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__learning_rate=1, multi__estimator__n_estimators=100, score=0.239879845515663, total= 4.1min\n",
      "[CV] multi__estimator__learning_rate=1, multi__estimator__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 80.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__learning_rate=1, multi__estimator__n_estimators=100, score=0.23844943498784152, total= 4.1min\n",
      "[CV] multi__estimator__learning_rate=1, multi__estimator__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed: 84.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__learning_rate=1, multi__estimator__n_estimators=100, score=0.2463519313304721, total= 4.1min\n",
      "[CV] multi__estimator__learning_rate=1, multi__estimator__n_estimators=200 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed: 88.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__learning_rate=1, multi__estimator__n_estimators=200, score=0.22743527392361607, total= 7.9min\n",
      "[CV] multi__estimator__learning_rate=1, multi__estimator__n_estimators=200 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed: 97.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__learning_rate=1, multi__estimator__n_estimators=200, score=0.22357316549849807, total= 8.0min\n",
      "[CV] multi__estimator__learning_rate=1, multi__estimator__n_estimators=200 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed: 105.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi__estimator__learning_rate=1, multi__estimator__n_estimators=200, score=0.23047210300429186, total= 8.0min\n",
      "[CV] multi__estimator__learning_rate=1, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__learning_rate=1, multi__estimator__n_estimators=300, score=0.19782577599771134, total=11.8min\n",
      "[CV] multi__estimator__learning_rate=1, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__learning_rate=1, multi__estimator__n_estimators=300, score=0.20912601916750106, total=11.9min\n",
      "[CV] multi__estimator__learning_rate=1, multi__estimator__n_estimators=300 \n",
      "[CV]  multi__estimator__learning_rate=1, multi__estimator__n_estimators=300, score=0.22203147353361946, total=12.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed: 152.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...mator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'multi__estimator__learning_rate': [0.9, 1], 'multi__estimator__n_estimators': [100, 200, 300]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'multi__estimator__learning_rate' : [0.9, 1],\n",
    "              'multi__estimator__n_estimators' : [100, 200, 300]}\n",
    "\n",
    "cv2 = GridSearchCV(alternate_pipeline,param_grid=parameters, verbose=15)\n",
    "cv2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 1. Classification report for related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.38      0.44      1206\n",
      "          1       0.82      0.89      0.86      4002\n",
      "          2       0.50      0.19      0.28        36\n",
      "\n",
      "avg / total       0.75      0.77      0.76      5244\n",
      "\n",
      "*** 2. Classification report for request\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.96      0.94      4317\n",
      "          1       0.76      0.58      0.66       927\n",
      "\n",
      "avg / total       0.89      0.89      0.89      5244\n",
      "\n",
      "*** 3. Classification report for offer\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      5220\n",
      "          1       0.00      0.00      0.00        24\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5244\n",
      "\n",
      "*** 4. Classification report for aid_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.85      0.81      3003\n",
      "          1       0.77      0.66      0.71      2241\n",
      "\n",
      "avg / total       0.77      0.77      0.77      5244\n",
      "\n",
      "*** 5. Classification report for medical_help\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.98      0.96      4817\n",
      "          1       0.56      0.28      0.37       427\n",
      "\n",
      "avg / total       0.91      0.92      0.91      5244\n",
      "\n",
      "*** 6. Classification report for medical_products\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.98      4966\n",
      "          1       0.66      0.34      0.45       278\n",
      "\n",
      "avg / total       0.95      0.96      0.95      5244\n",
      "\n",
      "*** 7. Classification report for search_and_rescue\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      5094\n",
      "          1       0.51      0.15      0.24       150\n",
      "\n",
      "avg / total       0.96      0.97      0.96      5244\n",
      "\n",
      "*** 8. Classification report for security\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      5140\n",
      "          1       0.42      0.05      0.09       104\n",
      "\n",
      "avg / total       0.97      0.98      0.97      5244\n",
      "\n",
      "*** 9. Classification report for military\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98      5069\n",
      "          1       0.48      0.30      0.37       175\n",
      "\n",
      "avg / total       0.96      0.97      0.96      5244\n",
      "\n",
      "*** 10. Classification report for child_alone\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      5244\n",
      "\n",
      "avg / total       1.00      1.00      1.00      5244\n",
      "\n",
      "*** 11. Classification report for water\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.98      0.98      4895\n",
      "          1       0.72      0.59      0.65       349\n",
      "\n",
      "avg / total       0.95      0.96      0.96      5244\n",
      "\n",
      "*** 12. Classification report for food\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.98      0.97      4640\n",
      "          1       0.86      0.72      0.78       604\n",
      "\n",
      "avg / total       0.95      0.95      0.95      5244\n",
      "\n",
      "*** 13. Classification report for shelter\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.98      0.97      4756\n",
      "          1       0.78      0.56      0.65       488\n",
      "\n",
      "avg / total       0.94      0.94      0.94      5244\n",
      "\n",
      "*** 14. Classification report for clothing\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5151\n",
      "          1       0.82      0.45      0.58        93\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5244\n",
      "\n",
      "*** 15. Classification report for money\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99      5128\n",
      "          1       0.54      0.31      0.39       116\n",
      "\n",
      "avg / total       0.97      0.98      0.98      5244\n",
      "\n",
      "*** 16. Classification report for missing_people\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5170\n",
      "          1       0.53      0.11      0.18        74\n",
      "\n",
      "avg / total       0.98      0.99      0.98      5244\n",
      "\n",
      "*** 17. Classification report for refugees\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98      5043\n",
      "          1       0.52      0.20      0.29       201\n",
      "\n",
      "avg / total       0.95      0.96      0.95      5244\n",
      "\n",
      "*** 18. Classification report for death\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98      4978\n",
      "          1       0.77      0.48      0.59       266\n",
      "\n",
      "avg / total       0.96      0.97      0.96      5244\n",
      "\n",
      "*** 19. Classification report for other_aid\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.97      0.92      4516\n",
      "          1       0.52      0.17      0.25       728\n",
      "\n",
      "avg / total       0.83      0.86      0.83      5244\n",
      "\n",
      "*** 20. Classification report for infrastructure_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.96      4901\n",
      "          1       0.35      0.11      0.17       343\n",
      "\n",
      "avg / total       0.90      0.93      0.91      5244\n",
      "\n",
      "*** 21. Classification report for transport\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.98      5001\n",
      "          1       0.59      0.21      0.31       243\n",
      "\n",
      "avg / total       0.95      0.96      0.95      5244\n",
      "\n",
      "*** 22. Classification report for buildings\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98      4987\n",
      "          1       0.65      0.46      0.54       257\n",
      "\n",
      "avg / total       0.96      0.96      0.96      5244\n",
      "\n",
      "*** 23. Classification report for electricity\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      5132\n",
      "          1       0.59      0.38      0.46       112\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5244\n",
      "\n",
      "*** 24. Classification report for tools\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00      5212\n",
      "          1       0.00      0.00      0.00        32\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5244\n",
      "\n",
      "*** 25. Classification report for hospitals\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5185\n",
      "          1       0.33      0.14      0.19        59\n",
      "\n",
      "avg / total       0.98      0.99      0.98      5244\n",
      "\n",
      "*** 26. Classification report for shops\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      5219\n",
      "          1       0.00      0.00      0.00        25\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5244\n",
      "\n",
      "*** 27. Classification report for aid_centers\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      5165\n",
      "          1       0.26      0.11      0.16        79\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5244\n",
      "\n",
      "*** 28. Classification report for other_infrastructure\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.97      5025\n",
      "          1       0.20      0.08      0.12       219\n",
      "\n",
      "avg / total       0.93      0.95      0.94      5244\n",
      "\n",
      "*** 29. Classification report for weather_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.95      0.91      3773\n",
      "          1       0.84      0.65      0.73      1471\n",
      "\n",
      "avg / total       0.86      0.87      0.86      5244\n",
      "\n",
      "*** 30. Classification report for floods\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.97      4796\n",
      "          1       0.84      0.51      0.64       448\n",
      "\n",
      "avg / total       0.95      0.95      0.94      5244\n",
      "\n",
      "*** 31. Classification report for storm\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.98      0.97      4763\n",
      "          1       0.74      0.55      0.63       481\n",
      "\n",
      "avg / total       0.94      0.94      0.94      5244\n",
      "\n",
      "*** 32. Classification report for fire\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      5181\n",
      "          1       0.65      0.21      0.31        63\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5244\n",
      "\n",
      "*** 33. Classification report for earthquake\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98      4766\n",
      "          1       0.86      0.77      0.81       478\n",
      "\n",
      "avg / total       0.97      0.97      0.97      5244\n",
      "\n",
      "*** 34. Classification report for cold\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      5150\n",
      "          1       0.56      0.37      0.45        94\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5244\n",
      "\n",
      "*** 35. Classification report for other_weather\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.99      0.97      4944\n",
      "          1       0.42      0.15      0.22       300\n",
      "\n",
      "avg / total       0.92      0.94      0.93      5244\n",
      "\n",
      "*** 36. Classification report for direct_report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.94      0.92      4193\n",
      "          1       0.70      0.53      0.60      1051\n",
      "\n",
      "avg / total       0.85      0.86      0.85      5244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = cv2.best_estimator_.predict(X_test)\n",
    "\n",
    "for i,col in enumerate(y_test.columns):\n",
    "    y_pred_i = y_pred.T[i]\n",
    "    y_test_i = y_test[col]\n",
    "    print('*** {}. Classification report for {}'.format(i+1,col))\n",
    "    print(classification_report(y_test_i, y_pred_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ator=None,\n",
       "          learning_rate=0.9, n_estimators=200, random_state=None),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'final_model_randomforest.sav'\n",
    "pickle.dump(cv.best_estimator_, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.694508009153\n"
     ]
    }
   ],
   "source": [
    "loaded_model1 = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model1.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'final_model_adaboost.sav'\n",
    "pickle.dump(cv2.best_estimator_, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.238939740656\n"
     ]
    }
   ],
   "source": [
    "loaded_model2 = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model2.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
